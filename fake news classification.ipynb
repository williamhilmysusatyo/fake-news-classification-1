{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nmZPji5UghG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow_model_analysis as tfma\n",
        "from tfx.components import (CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator,\n",
        "                            Transform, Trainer, Tuner, Evaluator, Pusher)\n",
        "from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "from tfx.dsl.components.common.resolver import Resolver\n",
        "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy\n",
        "from tfx.types import Channel\n",
        "from tfx.types.standard_artifacts import Model, ModelBlessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"pipelines\"\n",
        "SCHEMA_PIPELINE_NAME = \"fake-news-tfdv-schema\"\n",
        "SERVING_MODEL_DIR = \"serving_model\"\n",
        "\n",
        "# METADATA_PATH = os.path.join(\"metadata\", PIPELINE_NAME, \"metadata.db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "real_df = pd.read_csv(\"data/True.csv\")\n",
        "real_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fake_df = pd.read_csv(\"data/Fake.csv\")\n",
        "fake_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "real_df[\"class\"] = 1\n",
        "fake_df[\"class\"] = 0\n",
        "\n",
        "df = pd.concat([real_df, fake_df], axis=0)\n",
        "df = df.drop([\"title\", \"date\", \"subject\"], axis=1)\n",
        "df = df.rename(columns={\"class\": \"is_real\"})\n",
        "df.to_csv(\"data/csv/news.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_ROOT = \"data/csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactive_context = InteractiveContext(pipeline_root=PIPELINE_ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output = example_gen_pb2.Output(\n",
        "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
        "        example_gen_pb2.SplitConfig.Split(name=\"train\", hash_buckets=8),\n",
        "        example_gen_pb2.SplitConfig.Split(name=\"eval\", hash_buckets=2),\n",
        "    ])\n",
        ")\n",
        "\n",
        "example_gen = CsvExampleGen(input_base=DATA_ROOT, output_config=output)\n",
        "interactive_context.run(example_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs[\"examples\"]\n",
        ")\n",
        "interactive_context.run(statistics_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactive_context.show(statistics_gen.outputs[\"statistics\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "schema_gen = SchemaGen(\n",
        "    statistics=statistics_gen.outputs[\"statistics\"]\n",
        ")\n",
        "interactive_context.run(schema_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactive_context.show(schema_gen.outputs[\"schema\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_validator = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs[\"statistics\"],\n",
        "    schema=schema_gen.outputs[\"schema\"],\n",
        ")\n",
        "interactive_context.run(example_validator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactive_context.show(example_validator.outputs[\"anomalies\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRANSFORM_MODULE_FILE = \"news_transform.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile {TRANSFORM_MODULE_FILE}\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "LABEL_KEY = \"is_real\"\n",
        "FEATURE_KEY = \"text\"\n",
        "\n",
        "def transformed_name(key):\n",
        "    return f\"{key}_xf\"\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "    outputs = dict()\n",
        "    \n",
        "    outputs[transformed_name(FEATURE_KEY)] = tf.strings.lower(inputs[FEATURE_KEY])\n",
        "    outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)\n",
        "    \n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = Transform(\n",
        "    examples=example_gen.outputs[\"examples\"],\n",
        "    schema=schema_gen.outputs[\"schema\"],\n",
        "    module_file=os.path.abspath(TRANSFORM_MODULE_FILE)\n",
        ")\n",
        "interactive_context.run(transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAINER_MODULE_FILE = \"news_trainer.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile {TRAINER_MODULE_FILE}\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs\n",
        "\n",
        "LABEL_KEY = \"is_real\"\n",
        "FEATURE_KEY = \"text\"\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "def transformed_name(key): \n",
        "    return f\"{key}_xf\"\n",
        "\n",
        "def gzip_reader_fn(filenames):\n",
        "    return tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
        "\n",
        "def input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=64)->tf.data.Dataset:\n",
        "    transform_feature_spec = (\n",
        "        tf_transform_output.transformed_feature_spec().copy()\n",
        "    )\n",
        "    \n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=transform_feature_spec,\n",
        "        reader=gzip_reader_fn,\n",
        "        num_epochs=num_epochs,\n",
        "        label_key=transformed_name(LABEL_KEY),\n",
        "    )\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def model_builder(vectorizer_layer):\n",
        "    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)\n",
        "\n",
        "    x = vectorizer_layer(inputs)\n",
        "    x = layers.Embedding(input_dim=5000, output_dim=128)(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(128))(x)\n",
        "    x = layers.Dense(128, activation=tf.nn.relu)(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(64, activation=tf.nn.relu)(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=tf.nn.sigmoid)(x)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=inputs, outputs = outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy()],\n",
        "    )\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def _get_serve_tf_example_fn(model, tf_transform_output):\n",
        "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "    \n",
        "    @tf.function\n",
        "    def serve_tf_example_fn(serialized_tf_examples):\n",
        "        feature_spec = tf_transform_output.raw_feature_spec()\n",
        "        feature_spec.pop(LABEL_KEY)\n",
        "        \n",
        "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "        \n",
        "        # get predictions using transformed features\n",
        "        return model(transformed_features)\n",
        "    \n",
        "    return serve_tf_example_fn\n",
        "\n",
        "def run_fn(fn_args: FnArgs):\n",
        "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), \"logs\")\n",
        "    \n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=\"batch\")\n",
        "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_binary_accuracy\",\n",
        "        mode=\"max\",\n",
        "        verbose=1,\n",
        "        patience=10,\n",
        "    )\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        fn_args.serving_model_dir,\n",
        "        monitor=\"val_binary_accuracy\",\n",
        "        mode=\"max\",\n",
        "        verbose=1,\n",
        "        save_best_only=True,\n",
        "    )\n",
        "    callbacks = [\n",
        "        tensorboard_callback, \n",
        "        early_stopping_callback, \n",
        "        model_checkpoint_callback\n",
        "    ]\n",
        "    \n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "    \n",
        "    train_set = input_fn(fn_args.train_files, tf_transform_output, NUM_EPOCHS)\n",
        "    eval_set = input_fn(fn_args.eval_files, tf_transform_output, NUM_EPOCHS)\n",
        "    \n",
        "    vectorizer_dataset = train_set.map(lambda f, l: f[transformed_name(FEATURE_KEY)])\n",
        "    \n",
        "    vectorizer_layer = layers.TextVectorization(\n",
        "        max_tokens=5000,\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=40,\n",
        "    )\n",
        "    vectorizer_layer.adapt(vectorizer_dataset)\n",
        "    \n",
        "    model = model_builder(vectorizer_layer)\n",
        "    \n",
        "    model.fit(\n",
        "        x=train_set,\n",
        "        steps_per_epoch=fn_args.train_steps,\n",
        "        validation_data=eval_set,\n",
        "        validation_steps=fn_args.eval_steps,\n",
        "        callbacks=callbacks,\n",
        "        epochs=NUM_EPOCHS,\n",
        "        verbose=1,\n",
        "    )\n",
        "    \n",
        "    signatures = {\n",
        "        \"serving_default\": _get_serve_tf_example_fn(model, tf_transform_output).get_concrete_function(\n",
        "            tf.TensorSpec(\n",
        "                shape=[None],\n",
        "                dtype=tf.string,\n",
        "                name=\"examples\",\n",
        "            )\n",
        "        )\n",
        "    }\n",
        "    \n",
        "    model.save(fn_args.serving_model_dir, save_format=\"tf\", signatures=signatures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    module_file=os.path.abspath(TRAINER_MODULE_FILE),\n",
        "    examples=transform.outputs[\"transformed_examples\"],\n",
        "    transform_graph=transform.outputs[\"transform_graph\"],\n",
        "    schema=schema_gen.outputs[\"schema\"],\n",
        "    train_args=trainer_pb2.TrainArgs(splits=[\"train\"], num_steps=800),\n",
        "    eval_args=trainer_pb2.EvalArgs(splits=[\"eval\"], num_steps=400),\n",
        ")\n",
        "interactive_context.run(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TUNER_MODULE_FILE = \"news_tuner.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile {TUNER_MODULE_FILE}\n",
        "import keras_tuner as kt\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow.keras import layers\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs\n",
        "\n",
        "LABEL_KEY = \"emotions\"\n",
        "FEATURE_KEY = \"text\"\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "def transformed_name(key):\n",
        "    return f\"{key}_xf\"\n",
        "\n",
        "def gzip_reader_fn(filenames):\n",
        "    return tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
        "\n",
        "def input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=64)->tf.data.Dataset:\n",
        "    transform_feature_spec = (\n",
        "        tf_transform_output.transformed_feature_spec().copy()\n",
        "    )\n",
        "    \n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=transform_feature_spec,\n",
        "        reader=gzip_reader_fn,\n",
        "        num_epochs=num_epochs,\n",
        "        label_key=transformed_name(LABEL_KEY),\n",
        "    )\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def model_builder(vectorizer_layer):\n",
        "    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)\n",
        "\n",
        "    x = vectorizer_layer(inputs)\n",
        "    x = layers.Embedding(input_dim=5000, output_dim=16)(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(6, activation='softmax')(x)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=inputs, outputs = outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def tuner_fn(fn_args: FnArgs):\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "    \n",
        "    train_set = input_fn(fn_args.train_files, tf_transform_output, NUM_EPOCHS, 128)\n",
        "    eval_set = input_fn(fn_args.eval_files, tf_transform_output, NUM_EPOCHS, 128)\n",
        "    \n",
        "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_accuracy\",\n",
        "        mode=\"max\",\n",
        "        verbose=1,\n",
        "        patience=10,\n",
        "    )\n",
        "    vectorizer_dataset = train_set.map(lambda f, l: f[transformed_name(FEATURE_KEY)])\n",
        "    \n",
        "    vectorizer_layer = layers.TextVectorization(\n",
        "        max_tokens=5000,\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=20,\n",
        "    )\n",
        "    vectorizer_layer.adapt(vectorizer_dataset)\n",
        "    \n",
        "    tuner = kt.Hyperband(\n",
        "        hypermodel=model_builder(vectorizer_layer),\n",
        "        objective=\"val_accuracy\",\n",
        "        max_epochs=30,\n",
        "        factor=3,\n",
        "        directory=fn_args.working_dir,\n",
        "        project_name=\"kt_hyperband\",\n",
        "    )\n",
        "    \n",
        "    return TunerFnResult(\n",
        "        tuner=tuner,\n",
        "        fit_kwargs={\n",
        "            \"x\": train_set,\n",
        "            \"validation_data\": eval_set,\n",
        "            \"steps_per_epoch\": fn_args.train_steps,\n",
        "            \"validation_steps\": fn_args.eval_steps,\n",
        "            \"callbacks\": [early_stopping_callback],\n",
        "        },\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tuner = Tuner(\n",
        "    module_file=os.path.abspath(TUNER_MODULE_FILE),\n",
        "    examples=transform.outputs[\"transformed_examples\"],\n",
        "    transform_graph=transform.outputs[\"transform_graph\"],\n",
        "    schema=schema_gen.outputs[\"schema\"],\n",
        "    train_args=trainer_pb2.TrainArgs(splits=[\"train\"], num_steps=800),\n",
        "    eval_args=trainer_pb2.EvalArgs(splits=[\"eval\"], num_steps=400),\n",
        ")\n",
        "interactive_context.run(tuner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_resolver = Resolver(\n",
        "    strategy_class=LatestBlessedModelStrategy,\n",
        "    model=Channel(type=Model),\n",
        "    model_blessing=Channel(type=ModelBlessing),\n",
        ").with_id(\"Latest_blessed_model_resolver\")\n",
        "\n",
        "interactive_context.run(model_resolver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_config = tfma.EvalConfig(\n",
        "    model_specs=[tfma.ModelSpec(label_key=\"emotions\")],\n",
        "    slicing_specs=[tfma.SlicingSpec()],\n",
        "    metrics_specs=[\n",
        "        tfma.MetricsSpec(metrics=[\n",
        "            tfma.MetricConfig(class_name=\"ExampleCount\"),\n",
        "            tfma.MetricConfig(class_name=\"AUC\"),\n",
        "            tfma.MetricConfig(class_name=\"TruePositives\"),\n",
        "            tfma.MetricConfig(class_name=\"FalsePositives\"),\n",
        "            tfma.MetricConfig(class_name=\"TrueNegatives\"),\n",
        "            tfma.MetricConfig(class_name=\"FalseNegatives\"),\n",
        "            tfma.MetricConfig(class_name=\"Accuracy\", threshold=tfma.MetricThreshold(\n",
        "                value_threshold=tfma.GenericValueThreshold(\n",
        "                    lower_bound={\"value\": .6},\n",
        "                ),\n",
        "                change_threshold=tfma.GenericChangeThreshold(\n",
        "                    direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                    absolute={\"value\": 0.0001},\n",
        "                ),\n",
        "            )),\n",
        "        ])\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluator = Evaluator(\n",
        "    examples=example_gen.outputs[\"examples\"],\n",
        "    model=trainer.outputs[\"model\"],\n",
        "    baseline_model=model_resolver.outputs[\"model\"],\n",
        "    eval_config=eval_config,\n",
        ")\n",
        "interactive_context.run(evaluator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_result = evaluator.outputs[\"evaluation\"].get()[0].uri\n",
        "tfma_result = tfma.load_eval_result(eval_result)\n",
        "tfma.addons.fairness.view.widget_view.render_fainess_indicator(tfma_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pusher = Pusher(\n",
        "    model=trainer.outputs[\"model\"],\n",
        "    model_blessing=evaluator.outputs[\"blessing\"],\n",
        "    push_destination=pusher_pb2.PushDestination(\n",
        "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "            base_directory=SERVING_MODEL_DIR,\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "interactive_context.run(pusher)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SubmissionNLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "66cec7b311fdd594ef89c9790dd14ca2d44b4d36bfc6c6c38fd8ee89b9f699ad"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
