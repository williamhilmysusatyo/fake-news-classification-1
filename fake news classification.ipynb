{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nmZPji5UghG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow_model_analysis as tfma\n",
        "from tfx.components import (CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator,\n",
        "                            Transform, Trainer, Tuner, Evaluator, Pusher)\n",
        "from tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "from tfx.dsl.components.common.resolver import Resolver\n",
        "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy\n",
        "from tfx.types import Channel\n",
        "from tfx.types.standard_artifacts import Model, ModelBlessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"pipelines\"\n",
        "SCHEMA_PIPELINE_NAME = \"fake-news-tfdv-schema\"\n",
        "SERVING_MODEL_DIR = \"serving_model\"\n",
        "\n",
        "# METADATA_PATH = os.path.join(\"metadata\", PIPELINE_NAME, \"metadata.db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "real_df = pd.read_csv(\"data/True.csv\")\n",
        "real_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fake_df = pd.read_csv(\"data/Fake.csv\")\n",
        "fake_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "real_df[\"class\"] = 1\n",
        "fake_df[\"class\"] = 0\n",
        "\n",
        "df = pd.concat([real_df, fake_df], axis=0)\n",
        "df = df.drop([\"title\", \"date\", \"subject\"], axis=1)\n",
        "df = df.rename(columns={\"class\": \"is_real\"})\n",
        "df.to_csv(\"data/csv/news.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_ROOT = \"data/csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactive_context = InteractiveContext(pipeline_root=PIPELINE_ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output = example_gen_pb2.Output(\n",
        "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
        "        example_gen_pb2.SplitConfig.Split(name=\"train\", hash_buckets=8),\n",
        "        example_gen_pb2.SplitConfig.Split(name=\"eval\", hash_buckets=2),\n",
        "    ])\n",
        ")\n",
        "\n",
        "example_gen = CsvExampleGen(input_base=DATA_ROOT, output_config=output)\n",
        "interactive_context.run(example_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs[\"examples\"]\n",
        ")\n",
        "interactive_context.run(statistics_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactive_context.show(statistics_gen.outputs[\"statistics\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "schema_gen = SchemaGen(\n",
        "    statistics=statistics_gen.outputs[\"statistics\"]\n",
        ")\n",
        "interactive_context.run(schema_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactive_context.show(schema_gen.outputs[\"schema\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_validator = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs[\"statistics\"],\n",
        "    schema=schema_gen.outputs[\"schema\"],\n",
        ")\n",
        "interactive_context.run(example_validator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interactive_context.show(example_validator.outputs[\"anomalies\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRANSFORM_MODULE_FILE = \"news_transform.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile {TRANSFORM_MODULE_FILE}\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "LABEL_KEY = \"is_real\"\n",
        "FEATURE_KEY = \"text\"\n",
        "\n",
        "def transformed_name(key):\n",
        "    return f\"{key}_xf\"\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "    outputs = dict()\n",
        "    \n",
        "    outputs[transformed_name(FEATURE_KEY)] = tf.strings.lower(inputs[FEATURE_KEY])\n",
        "    outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)\n",
        "    \n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [17], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m transform \u001b[39m=\u001b[39m Transform(\n\u001b[0;32m      2\u001b[0m     examples\u001b[39m=\u001b[39mexample_gen\u001b[39m.\u001b[39moutputs[\u001b[39m\"\u001b[39m\u001b[39mexamples\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m      3\u001b[0m     schema\u001b[39m=\u001b[39mschema_gen\u001b[39m.\u001b[39moutputs[\u001b[39m\"\u001b[39m\u001b[39mschema\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m      4\u001b[0m     module_file\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(TRANSFORM_MODULE_FILE)\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 6\u001b[0m interactive_context\u001b[39m.\u001b[39;49mrun(transform)\n",
            "File \u001b[1;32mc:\\Users\\Zis\\anaconda3\\lib\\site-packages\\tfx\\orchestration\\experimental\\interactive\\notebook_utils.py:31\u001b[0m, in \u001b[0;36mrequires_ipython.<locals>.run_if_ipython\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39m\"\"\"Invokes `fn` if called from IPython, otherwise just emits a warning.\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(builtins, \u001b[39m'\u001b[39m\u001b[39m__IPYTHON__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     29\u001b[0m   \u001b[39m# __IPYTHON__ variable is set by IPython, see\u001b[39;00m\n\u001b[0;32m     30\u001b[0m   \u001b[39m# https://ipython.org/ipython-doc/rel-0.10.2/html/interactive/reference.html#embedding-ipython.\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m   logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m     34\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mMethod \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is a no-op when invoked outside of IPython.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     35\u001b[0m       fn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Zis\\anaconda3\\lib\\site-packages\\tfx\\orchestration\\experimental\\interactive\\interactive_context.py:164\u001b[0m, in \u001b[0;36mInteractiveContext.run\u001b[1;34m(self, component, enable_cache, beam_pipeline_args)\u001b[0m\n\u001b[0;32m    160\u001b[0m   runner_label \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39minteractivecontext\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    161\u001b[0m \u001b[39mwith\u001b[39;00m telemetry_utils\u001b[39m.\u001b[39mscoped_labels({\n\u001b[0;32m    162\u001b[0m     telemetry_utils\u001b[39m.\u001b[39mLABEL_TFX_RUNNER: runner_label,\n\u001b[0;32m    163\u001b[0m }):\n\u001b[1;32m--> 164\u001b[0m   execution_id \u001b[39m=\u001b[39m launcher\u001b[39m.\u001b[39;49mlaunch()\u001b[39m.\u001b[39mexecution_id\n\u001b[0;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m execution_result\u001b[39m.\u001b[39mExecutionResult(\n\u001b[0;32m    167\u001b[0m     component\u001b[39m=\u001b[39mcomponent, execution_id\u001b[39m=\u001b[39mexecution_id)\n",
            "File \u001b[1;32mc:\\Users\\Zis\\anaconda3\\lib\\site-packages\\tfx\\orchestration\\launcher\\base_component_launcher.py:200\u001b[0m, in \u001b[0;36mBaseComponentLauncher.launch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m   absl\u001b[39m.\u001b[39mlogging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mRunning executor for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m    196\u001b[0m                     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_component_info\u001b[39m.\u001b[39mcomponent_id)\n\u001b[0;32m    197\u001b[0m   \u001b[39m# Make a deep copy for input_dict and exec_properties, because they should\u001b[39;00m\n\u001b[0;32m    198\u001b[0m   \u001b[39m# be immutable in this context.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m   \u001b[39m# output_dict can still be changed, specifically properties.\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_executor(execution_decision\u001b[39m.\u001b[39;49mexecution_id,\n\u001b[0;32m    201\u001b[0m                      copy\u001b[39m.\u001b[39;49mdeepcopy(execution_decision\u001b[39m.\u001b[39;49minput_dict),\n\u001b[0;32m    202\u001b[0m                      execution_decision\u001b[39m.\u001b[39;49moutput_dict,\n\u001b[0;32m    203\u001b[0m                      copy\u001b[39m.\u001b[39;49mdeepcopy(execution_decision\u001b[39m.\u001b[39;49mexec_properties))\n\u001b[0;32m    205\u001b[0m absl\u001b[39m.\u001b[39mlogging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mRunning publisher for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m    206\u001b[0m                   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_component_info\u001b[39m.\u001b[39mcomponent_id)\n\u001b[0;32m    207\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_publisher(output_dict\u001b[39m=\u001b[39mexecution_decision\u001b[39m.\u001b[39moutput_dict)\n",
            "File \u001b[1;32mc:\\Users\\Zis\\anaconda3\\lib\\site-packages\\tfx\\orchestration\\launcher\\in_process_component_launcher.py:73\u001b[0m, in \u001b[0;36mInProcessComponentLauncher._run_executor\u001b[1;34m(self, execution_id, input_dict, output_dict, exec_properties)\u001b[0m\n\u001b[0;32m     67\u001b[0m executor \u001b[39m=\u001b[39m executor_class_spec\u001b[39m.\u001b[39mexecutor_class(\n\u001b[0;32m     68\u001b[0m     executor_context)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39m# Make a deep copy for input_dict and exec_properties, because they should\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m# be immutable in this context.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39m# output_dict can still be changed, specifically properties.\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m executor\u001b[39m.\u001b[39;49mDo(\n\u001b[0;32m     74\u001b[0m     copy\u001b[39m.\u001b[39;49mdeepcopy(input_dict), output_dict, copy\u001b[39m.\u001b[39;49mdeepcopy(exec_properties))\n",
            "File \u001b[1;32mc:\\Users\\Zis\\anaconda3\\lib\\site-packages\\tfx\\components\\transform\\executor.py:586\u001b[0m, in \u001b[0;36mExecutor.Do\u001b[1;34m(self, input_dict, output_dict, exec_properties)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[39m# TempPipInstallContext is needed here so that subprocesses (which\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \u001b[39m# may be created by the Beam multi-process DirectRunner) can find the\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \u001b[39m# needed dependencies.\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \u001b[39m# TODO(b/187122662): Move this to the ExecutorOperator or Launcher and\u001b[39;00m\n\u001b[0;32m    584\u001b[0m \u001b[39m# remove the `_pip_dependencies` attribute.\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \u001b[39mwith\u001b[39;00m udf_utils\u001b[39m.\u001b[39mTempPipInstallContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pip_dependencies):\n\u001b[1;32m--> 586\u001b[0m   TransformProcessor()\u001b[39m.\u001b[39;49mTransform(label_inputs, label_outputs, status_file)\n\u001b[0;32m    587\u001b[0m logging\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mCleaning up temp path \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m on executor success\u001b[39m\u001b[39m'\u001b[39m, temp_path)\n\u001b[0;32m    588\u001b[0m io_utils\u001b[39m.\u001b[39mdelete_dir(temp_path)\n",
            "File \u001b[1;32mc:\\Users\\Zis\\anaconda3\\lib\\site-packages\\tfx\\components\\transform\\executor.py:1138\u001b[0m, in \u001b[0;36mTransformProcessor.Transform\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1133\u001b[0m stats_options_updater_fn \u001b[39m=\u001b[39m (stats_options_updater_fn\n\u001b[0;32m   1134\u001b[0m                             \u001b[39mif\u001b[39;00m stats_options_updater_fn \u001b[39melse\u001b[39;00m \u001b[39mlambda\u001b[39;00m _, x: x)\n\u001b[0;32m   1136\u001b[0m materialization_format \u001b[39m=\u001b[39m (\n\u001b[0;32m   1137\u001b[0m     transform_paths_file_formats[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m materialize_output_paths \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 1138\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_RunBeamImpl(analyze_data_list, transform_data_list, preprocessing_fn,\n\u001b[0;32m   1139\u001b[0m                   stats_options_updater_fn, force_tf_compat_v1,\n\u001b[0;32m   1140\u001b[0m                   input_dataset_metadata, transform_output_path,\n\u001b[0;32m   1141\u001b[0m                   raw_examples_data_format, temp_path, input_cache_dir,\n\u001b[0;32m   1142\u001b[0m                   output_cache_dir, disable_statistics,\n\u001b[0;32m   1143\u001b[0m                   per_set_stats_output_paths, materialization_format,\n\u001b[0;32m   1144\u001b[0m                   \u001b[39mlen\u001b[39;49m(analyze_data_paths), stats_output_paths,\n\u001b[0;32m   1145\u001b[0m                   make_beam_pipeline_fn)\n",
            "File \u001b[1;32mc:\\Users\\Zis\\anaconda3\\lib\\site-packages\\tfx\\components\\transform\\executor.py:1229\u001b[0m, in \u001b[0;36mTransformProcessor._RunBeamImpl\u001b[1;34m(self, analyze_data_list, transform_data_list, preprocessing_fn, stats_options_updater_fn, force_tf_compat_v1, input_dataset_metadata, transform_output_path, raw_examples_data_format, temp_path, input_cache_dir, output_cache_dir, disable_statistics, per_set_stats_output_paths, materialization_format, analyze_paths_count, stats_output_paths, make_beam_pipeline_fn)\u001b[0m\n\u001b[0;32m   1224\u001b[0m   d\u001b[39m.\u001b[39mtfxio \u001b[39m=\u001b[39m d\u001b[39m.\u001b[39mtfxio\u001b[39m.\u001b[39mProject(transform_input_columns)\n\u001b[0;32m   1226\u001b[0m desired_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_GetDesiredBatchSize(\n\u001b[0;32m   1227\u001b[0m     raw_examples_data_format, input_dataset_metadata\u001b[39m.\u001b[39mschema)\n\u001b[1;32m-> 1229\u001b[0m \u001b[39mwith\u001b[39;00m make_beam_pipeline_fn() \u001b[39mas\u001b[39;00m pipeline:\n\u001b[0;32m   1230\u001b[0m   \u001b[39mwith\u001b[39;00m tft_beam\u001b[39m.\u001b[39mContext(\n\u001b[0;32m   1231\u001b[0m       temp_dir\u001b[39m=\u001b[39mtemp_path,\n\u001b[0;32m   1232\u001b[0m       desired_batch_size\u001b[39m=\u001b[39mdesired_batch_size,\n\u001b[0;32m   1233\u001b[0m       passthrough_keys\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_GetTFXIOPassthroughKeys(),\n\u001b[0;32m   1234\u001b[0m       use_deep_copy_optimization\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   1235\u001b[0m       force_tf_compat_v1\u001b[39m=\u001b[39mforce_tf_compat_v1):\n\u001b[0;32m   1236\u001b[0m     (new_analyze_data_dict, input_cache,\n\u001b[0;32m   1237\u001b[0m      estimated_stage_count_with_cache) \u001b[39m=\u001b[39m (\n\u001b[0;32m   1238\u001b[0m          pipeline\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1241\u001b[0m              unprojected_typespecs, preprocessing_fn,\n\u001b[0;32m   1242\u001b[0m              \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_GetCacheSource(), force_tf_compat_v1))\n",
            "File \u001b[1;32mc:\\Users\\Zis\\anaconda3\\lib\\site-packages\\tfx\\dsl\\components\\base\\base_beam_executor.py:104\u001b[0m, in \u001b[0;36mBaseBeamExecutor._make_beam_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m beam:\n\u001b[0;32m    101\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[0;32m    102\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mApache Beam must be installed to use this functionality.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 104\u001b[0m result \u001b[39m=\u001b[39m beam\u001b[39m.\u001b[39;49mPipeline(argv\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_beam_pipeline_args)\n\u001b[0;32m    106\u001b[0m \u001b[39m# TODO(b/159468583): Obivate this code block by moving the warning to Beam.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39m# pytype: disable=import-error\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mapache_beam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptions\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline_options\u001b[39;00m \u001b[39mimport\u001b[39;00m DirectOptions\n",
            "File \u001b[1;32mc:\\Users\\Zis\\anaconda3\\lib\\site-packages\\apache_beam\\pipeline.py:198\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[1;34m(self, runner, options, argv)\u001b[0m\n\u001b[0;32m    193\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mRunner \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not a PipelineRunner object or the \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    195\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mname of a registered runner.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m runner)\n\u001b[0;32m    197\u001b[0m \u001b[39m# Validate pipeline options\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m errors \u001b[39m=\u001b[39m PipelineOptionsValidator(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_options, runner)\u001b[39m.\u001b[39;49mvalidate()\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m errors:\n\u001b[0;32m    200\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mPipeline has validations errors: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(errors))\n",
            "File \u001b[1;32mc:\\Users\\Zis\\anaconda3\\lib\\site-packages\\apache_beam\\options\\pipeline_options_validator.py:146\u001b[0m, in \u001b[0;36mPipelineOptionsValidator.validate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mOPTIONS:\n\u001b[0;32m    145\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mvalidate\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39mand\u001b[39;00m callable(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mvalidate\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m--> 146\u001b[0m     errors\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mview_as(\u001b[39mcls\u001b[39;49m)\u001b[39m.\u001b[39;49mvalidate(\u001b[39mself\u001b[39;49m))\n\u001b[0;32m    147\u001b[0m \u001b[39mreturn\u001b[39;00m errors\n",
            "File \u001b[1;32mc:\\Users\\Zis\\anaconda3\\lib\\site-packages\\apache_beam\\options\\pipeline_options.py:1486\u001b[0m, in \u001b[0;36mTestOptions.validate\u001b[1;34m(self, validator)\u001b[0m\n\u001b[0;32m   1484\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate\u001b[39m(\u001b[39mself\u001b[39m, validator):\n\u001b[0;32m   1485\u001b[0m   errors \u001b[39m=\u001b[39m []\n\u001b[1;32m-> 1486\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mview_as(TestOptions)\u001b[39m.\u001b[39mon_success_matcher:\n\u001b[0;32m   1487\u001b[0m     errors\u001b[39m.\u001b[39mextend(validator\u001b[39m.\u001b[39mvalidate_test_matcher(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mon_success_matcher\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m   1488\u001b[0m   \u001b[39mreturn\u001b[39;00m errors\n",
            "File \u001b[1;32mc:\\Users\\Zis\\anaconda3\\lib\\site-packages\\apache_beam\\options\\pipeline_options.py:390\u001b[0m, in \u001b[0;36mPipelineOptions.view_as\u001b[1;34m(self, cls)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[39m\"\"\"Returns a view of current object as provided PipelineOption subclass.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \n\u001b[0;32m    369\u001b[0m \u001b[39mExample Usage::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    386\u001b[0m \n\u001b[0;32m    387\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    388\u001b[0m view \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flags)\n\u001b[1;32m--> 390\u001b[0m \u001b[39mfor\u001b[39;00m option_name \u001b[39min\u001b[39;00m view\u001b[39m.\u001b[39;49m_visible_option_list():\n\u001b[0;32m    391\u001b[0m   \u001b[39m# Initialize values of keys defined by a cls.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m   \u001b[39m#\u001b[39;00m\n\u001b[0;32m    393\u001b[0m   \u001b[39m# Note that we do initialization only once per key to make sure that\u001b[39;00m\n\u001b[0;32m    394\u001b[0m   \u001b[39m# values in _all_options dict are not-recreated with each new view.\u001b[39;00m\n\u001b[0;32m    395\u001b[0m   \u001b[39m# This is important to make sure that values of multi-options keys are\u001b[39;00m\n\u001b[0;32m    396\u001b[0m   \u001b[39m# backed by the same list across multiple views, and that any overrides of\u001b[39;00m\n\u001b[0;32m    397\u001b[0m   \u001b[39m# pipeline options already stored in _all_options are preserved.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m   \u001b[39mif\u001b[39;00m option_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_all_options:\n\u001b[0;32m    399\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_all_options[option_name] \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\n\u001b[0;32m    400\u001b[0m         view\u001b[39m.\u001b[39m_visible_options, option_name)\n",
            "File \u001b[1;32mc:\\Users\\Zis\\anaconda3\\lib\\site-packages\\apache_beam\\options\\pipeline_options.py:407\u001b[0m, in \u001b[0;36mPipelineOptions._visible_option_list\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_visible_option_list\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    406\u001b[0m   \u001b[39m# type: () -> List[str]\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msorted\u001b[39m(\n\u001b[0;32m    408\u001b[0m       option \u001b[39mfor\u001b[39;00m option \u001b[39min\u001b[39;00m \u001b[39mdir\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_visible_options) \u001b[39mif\u001b[39;00m option[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "transform = Transform(\n",
        "    examples=example_gen.outputs[\"examples\"],\n",
        "    schema=schema_gen.outputs[\"schema\"],\n",
        "    module_file=os.path.abspath(TRANSFORM_MODULE_FILE)\n",
        ")\n",
        "interactive_context.run(transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAINER_MODULE_FILE = \"news_trainer.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile {TRAINER_MODULE_FILE}\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs\n",
        "\n",
        "LABEL_KEY = \"is_real\"\n",
        "FEATURE_KEY = \"text\"\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "def transformed_name(key): \n",
        "    return f\"{key}_xf\"\n",
        "\n",
        "def gzip_reader_fn(filenames):\n",
        "    return tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
        "\n",
        "def input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=64)->tf.data.Dataset:\n",
        "    transform_feature_spec = (\n",
        "        tf_transform_output.transformed_feature_spec().copy()\n",
        "    )\n",
        "    \n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=transform_feature_spec,\n",
        "        reader=gzip_reader_fn,\n",
        "        num_epochs=num_epochs,\n",
        "        label_key=transformed_name(LABEL_KEY),\n",
        "    )\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def model_builder(vectorizer_layer):\n",
        "    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)\n",
        "\n",
        "    x = vectorizer_layer(inputs)\n",
        "    x = layers.Embedding(input_dim=5000, output_dim=128)(x)\n",
        "    x = layers.LSTM(128)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(6, activation='softmax')(x)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=inputs, outputs = outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy()],\n",
        "    )\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def _get_serve_tf_example_fn(model, tf_transform_output):\n",
        "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "    \n",
        "    @tf.function\n",
        "    def serve_tf_example_fn(serialized_tf_examples):\n",
        "        feature_spec = tf_transform_output.raw_feature_spec()\n",
        "        feature_spec.pop(LABEL_KEY)\n",
        "        \n",
        "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "        \n",
        "        # get predictions using transformed features\n",
        "        return model(transformed_features)\n",
        "    \n",
        "    return serve_tf_example_fn\n",
        "\n",
        "def run_fn(fn_args: FnArgs):\n",
        "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), \"logs\")\n",
        "    \n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, update_freq=\"batch\")\n",
        "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_binary_accuracy\",\n",
        "        mode=\"max\",\n",
        "        verbose=1,\n",
        "        patience=10,\n",
        "    )\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        fn_args.serving_model_dir,\n",
        "        monitor=\"val_binary_accuracy\",\n",
        "        mode=\"max\",\n",
        "        verbose=1,\n",
        "        save_best_only=True,\n",
        "    )\n",
        "    callbacks = [\n",
        "        tensorboard_callback, \n",
        "        early_stopping_callback, \n",
        "        model_checkpoint_callback\n",
        "    ]\n",
        "    \n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "    \n",
        "    train_set = input_fn(fn_args.train_files, tf_transform_output, NUM_EPOCHS)\n",
        "    eval_set = input_fn(fn_args.eval_files, tf_transform_output, NUM_EPOCHS)\n",
        "    \n",
        "    vectorizer_dataset = train_set.map(lambda f, l: f[transformed_name(FEATURE_KEY)])\n",
        "    \n",
        "    vectorizer_layer = layers.TextVectorization(\n",
        "        max_tokens=5000,\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=40,\n",
        "    )\n",
        "    vectorizer_layer.adapt(vectorizer_dataset)\n",
        "    \n",
        "    model = model_builder(vectorizer_layer)\n",
        "    \n",
        "    model.fit(\n",
        "        x=train_set,\n",
        "        validation_data=eval_set,\n",
        "        callbacks=callbacks,\n",
        "        epochs=NUM_EPOCHS,\n",
        "        verbose=1,\n",
        "    )\n",
        "    \n",
        "    signatures = {\n",
        "        \"serving_default\": _get_serve_tf_example_fn(model, tf_transform_output).get_concrete_function(\n",
        "            tf.TensorSpec(\n",
        "                shape=[None],\n",
        "                dtype=tf.string,\n",
        "                name=\"examples\",\n",
        "            )\n",
        "        )\n",
        "    }\n",
        "    \n",
        "    model.save(fn_args.serving_model_dir, save_format=\"tf\", signatures=signatures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    module_file=os.path.abspath(TRAINER_MODULE_FILE),\n",
        "    examples=transform.outputs[\"transformed_examples\"],\n",
        "    transform_graph=transform.outputs[\"transform_graph\"],\n",
        "    schema=schema_gen.outputs[\"schema\"],\n",
        "    train_args=trainer_pb2.TrainArgs(splits=[\"train\"]),\n",
        "    eval_args=trainer_pb2.EvalArgs(splits=[\"eval\"]),\n",
        ")\n",
        "interactive_context.run(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TUNER_MODULE_FILE = \"news_tuner.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile {TUNER_MODULE_FILE}\n",
        "import keras_tuner as kt\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow.keras import layers\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs\n",
        "\n",
        "LABEL_KEY = \"emotions\"\n",
        "FEATURE_KEY = \"text\"\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "def transformed_name(key):\n",
        "    return f\"{key}_xf\"\n",
        "\n",
        "def gzip_reader_fn(filenames):\n",
        "    return tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
        "\n",
        "def input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=64)->tf.data.Dataset:\n",
        "    transform_feature_spec = (\n",
        "        tf_transform_output.transformed_feature_spec().copy()\n",
        "    )\n",
        "    \n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=transform_feature_spec,\n",
        "        reader=gzip_reader_fn,\n",
        "        num_epochs=num_epochs,\n",
        "        label_key=transformed_name(LABEL_KEY),\n",
        "    )\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def model_builder(vectorizer_layer):\n",
        "    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)\n",
        "\n",
        "    x = vectorizer_layer(inputs)\n",
        "    x = layers.Embedding(input_dim=5000, output_dim=16)(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(6, activation='softmax')(x)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=inputs, outputs = outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def tuner_fn(fn_args: FnArgs):\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "    \n",
        "    train_set = input_fn(fn_args.train_files, tf_transform_output, NUM_EPOCHS, 128)\n",
        "    eval_set = input_fn(fn_args.eval_files, tf_transform_output, NUM_EPOCHS, 128)\n",
        "    \n",
        "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_accuracy\",\n",
        "        mode=\"max\",\n",
        "        verbose=1,\n",
        "        patience=10,\n",
        "    )\n",
        "    vectorizer_dataset = train_set.map(lambda f, l: f[transformed_name(FEATURE_KEY)])\n",
        "    \n",
        "    vectorizer_layer = layers.TextVectorization(\n",
        "        max_tokens=5000,\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=20,\n",
        "    )\n",
        "    vectorizer_layer.adapt(vectorizer_dataset)\n",
        "    \n",
        "    tuner = kt.Hyperband(\n",
        "        hypermodel=model_builder(vectorizer_layer),\n",
        "        objective=\"val_accuracy\",\n",
        "        max_epochs=30,\n",
        "        factor=3,\n",
        "        directory=fn_args.working_dir,\n",
        "        project_name=\"kt_hyperband\",\n",
        "    )\n",
        "    \n",
        "    return TunerFnResult(\n",
        "        tuner=tuner,\n",
        "        fit_kwargs={\n",
        "            \"x\": train_set,\n",
        "            \"validation_data\": eval_set,\n",
        "            \"steps_per_epoch\": fn_args.train_steps,\n",
        "            \"validation_steps\": fn_args.eval_steps,\n",
        "            \"callbacks\": [early_stopping_callback],\n",
        "        },\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tuner = Tuner(\n",
        "    module_file=os.path.abspath(TUNER_MODULE_FILE),\n",
        "    examples=transform.outputs[\"transformed_examples\"],\n",
        "    transform_graph=transform.outputs[\"transform_graph\"],\n",
        "    schema=schema_gen.outputs[\"schema\"],\n",
        "    train_args=trainer_pb2.TrainArgs(splits=[\"train\"], num_steps=800),\n",
        "    eval_args=trainer_pb2.EvalArgs(splits=[\"eval\"], num_steps=400),\n",
        ")\n",
        "interactive_context.run(tuner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_resolver = Resolver(\n",
        "    strategy_class=LatestBlessedModelStrategy,\n",
        "    model=Channel(type=Model),\n",
        "    model_blessing=Channel(type=ModelBlessing),\n",
        ").with_id(\"Latest_blessed_model_resolver\")\n",
        "\n",
        "interactive_context.run(model_resolver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_config = tfma.EvalConfig(\n",
        "    model_specs=[tfma.ModelSpec(label_key=\"emotions\")],\n",
        "    slicing_specs=[tfma.SlicingSpec()],\n",
        "    metrics_specs=[\n",
        "        tfma.MetricsSpec(metrics=[\n",
        "            tfma.MetricConfig(class_name=\"ExampleCount\"),\n",
        "            tfma.MetricConfig(class_name=\"AUC\"),\n",
        "            tfma.MetricConfig(class_name=\"TruePositives\"),\n",
        "            tfma.MetricConfig(class_name=\"FalsePositives\"),\n",
        "            tfma.MetricConfig(class_name=\"TrueNegatives\"),\n",
        "            tfma.MetricConfig(class_name=\"FalseNegatives\"),\n",
        "            tfma.MetricConfig(class_name=\"Accuracy\", threshold=tfma.MetricThreshold(\n",
        "                value_threshold=tfma.GenericValueThreshold(\n",
        "                    lower_bound={\"value\": .6},\n",
        "                ),\n",
        "                change_threshold=tfma.GenericChangeThreshold(\n",
        "                    direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                    absolute={\"value\": 0.0001},\n",
        "                ),\n",
        "            )),\n",
        "        ])\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluator = Evaluator(\n",
        "    examples=example_gen.outputs[\"examples\"],\n",
        "    model=trainer.outputs[\"model\"],\n",
        "    baseline_model=model_resolver.outputs[\"model\"],\n",
        "    eval_config=eval_config,\n",
        ")\n",
        "interactive_context.run(evaluator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_result = evaluator.outputs[\"evaluation\"].get()[0].uri\n",
        "tfma_result = tfma.load_eval_result(eval_result)\n",
        "tfma.addons.fairness.view.widget_view.render_fainess_indicator(tfma_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pusher = Pusher(\n",
        "    model=trainer.outputs[\"model\"],\n",
        "    model_blessing=evaluator.outputs[\"blessing\"],\n",
        "    push_destination=pusher_pb2.PushDestination(\n",
        "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "            base_directory=SERVING_MODEL_DIR,\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "interactive_context.run(pusher)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SubmissionNLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "66cec7b311fdd594ef89c9790dd14ca2d44b4d36bfc6c6c38fd8ee89b9f699ad"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
